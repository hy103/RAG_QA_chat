{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshayarravarapu/Documents/GitHub/RAG_QA_chat/rag_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def extract_text_layout(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    full_text = []\n",
    "    for page in doc:\n",
    "        html = page.get_text(\"html\")\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        full_text.append(soup.get_text(separator=\" \"))\n",
    "    return \"\\n\".join(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = extract_text_layout(\"data/PC_US_Elections.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshayarravarapu/Documents/GitHub/RAG_QA_chat/rag_env/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:732: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/Users/harshayarravarapu/Documents/GitHub/RAG_QA_chat/rag_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total chunks: 45\n",
      "üßæ Sample chunk:\n",
      "  INSTITUTIONAL EQUITY RESEARCH  \n",
      " Page | 1 | PHILLIPCAPITAL INDIA RESEARCH  \n",
      " DISCLAIMER FOR U.S. BASED INVESTORS . The Agent of  PhillipCapital (India) Pvt. Ltd.  in the United States is Marco Polo Securities Inc, a non-affiliated broker-dealer registered with the US Securities and  \n",
      " Exchange Commission. The activities of  PhillipCapital (India) Pvt. Ltd.  in the United States will be affected only to the extent permitted by Rule 15a-6 under the US Securities Exchange Act of 1934 and in  \n",
      " acc\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Use LLaMA-2 tokenizer instead of FLAN-T5\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", use_auth_token=True)\n",
    "\n",
    "def smart_chunk_text(text, max_tokens=512, overlap_tokens=50):\n",
    "    paragraphs = text.split(\"\\n\")\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_token_count = 0\n",
    "\n",
    "    for para in paragraphs:\n",
    "        if not para.strip():\n",
    "            continue\n",
    "\n",
    "        para_tokens = tokenizer(para)[\"input_ids\"]\n",
    "        if current_token_count + len(para_tokens) <= max_tokens:\n",
    "            current_chunk.append(para)\n",
    "            current_token_count += len(para_tokens)\n",
    "        else:\n",
    "            combined = \"\\n\".join(current_chunk)\n",
    "            chunks.append(combined)\n",
    "\n",
    "            # Handle overlap\n",
    "            if overlap_tokens > 0 and chunks:\n",
    "                last_chunk_tokens = tokenizer(combined)[\"input_ids\"][-overlap_tokens:]\n",
    "                overlap_text = tokenizer.decode(last_chunk_tokens)\n",
    "                current_chunk = [overlap_text, para]\n",
    "                current_token_count = len(tokenizer(\" \".join(current_chunk))[\"input_ids\"])\n",
    "            else:\n",
    "                current_chunk = [para]\n",
    "                current_token_count = len(para_tokens)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\"\\n\".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Run on your parsed PDF text\n",
    "chunks = smart_chunk_text(pdf_text)\n",
    "print(f\"‚úÖ Total chunks: {len(chunks)}\")\n",
    "print(\"üßæ Sample chunk:\\n\", chunks[0][:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class Embedder:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_chunks(self, chunks):\n",
    "        return self.model.encode(\n",
    "            chunks,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=True\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "embedder = Embedder()\n",
    "embeddings = embedder.embed_chunks(chunks)\n",
    "print(\"‚úÖ Embeddings shape:\", embeddings.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "print(\"‚úÖ FAISS index created and populated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_chunks_for_query(query, top_k=3):\n",
    "    query_vec = embedder.model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "    D, I = index.search(query_vec, top_k)\n",
    "    return [chunks[i] for i in I[0]]\n",
    "\n",
    "query = \"What are the Biden policies?\"\n",
    "retrieved_chunks = retrieve_chunks_for_query(query)\n",
    "print(\"üîç Retrieved Chunks:\\n\")\n",
    "for i, chunk in enumerate(retrieved_chunks):\n",
    "    print(f\"Chunk {i+1}:\", chunk[:300], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Cell 7: Use LLaMA-2 to Generate the Answer\n",
    "class LLM:\n",
    "    def __init__(self, model_id=\"meta-llama/Llama-2-7b-chat-hf\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=\"auto\",\n",
    "            use_auth_token=True\n",
    "        )\n",
    "        self.generator = pipeline(\"text-generation\", model=self.model, tokenizer=self.tokenizer)\n",
    "\n",
    "    def answer_query(self, query, context_chunks):\n",
    "        context = \"\\n\".join(context_chunks)\n",
    "        prompt = f\"\"\"<s>[INST] <<SYS>>Use the following context to answer the question as accurately as possible.<<SYS>>\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer: [/INST]\"\"\"\n",
    "        \n",
    "        result = self.generator(prompt, max_new_tokens=256, max_length=1024, do_sample=False)[0][\"generated_text\"]\n",
    "        return result\n",
    "\n",
    "llm_model = LLM()\n",
    "answer = llm_model.answer_query(query, retrieved_chunks)\n",
    "print(\"üß† LLaMA-2 Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
